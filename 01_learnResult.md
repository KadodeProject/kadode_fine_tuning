# 学習に関しての検証結果

## 1.rinna/japanese-gpt2-medium を元に usuyuki 日記 2MB(日付＋本文＋タイトル)を学習させた場合

### 概要

日記の日付と本文からタイトルを生成するモデルを作るために、usuyuki のすべての日記を食わせて fine tuning を行った。  
結果、(当たり前だが)usuyuki の個人情報が反映されすぎたモデルができた。  
また精度に関しても日記の本文が一部のみ加味されたタイトルとなっており、個人利用としても実用性に欠ける結果となった。

```
Error. Human is Dead, mismatch.
```

### 学習

タスク：

| タスク                                           | 日記の日付と本文からタイトルを生成する                                                                                                                                                                                                                                                                                                                 |
| :----------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| fine tuning に用いたデータ                       | usuyuki の 2018 年 1 月 10 日から 2022 年 10 月 24 日までの 1275 日記(日付、タイトル、本文)、UTF8 で 2MB 程度                                                                                                                                                                                                                                          |
| 根幹となるモデル                                 | GPT-2                                                                                                                                                                                                                                                                                                                                                  |
| pre training とした事前学習済モデル              | [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium?text=%E7%94%9F%E5%91%BD%E3%80%81%E5%AE%87%E5%AE%99%E3%80%81%E3%81%9D%E3%81%97%E3%81%A6%E4%B8%87%E7%89%A9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E7%A9%B6%E6%A5%B5%E3%81%AE%E7%96%91%E5%95%8F%E3%81%AE%E7%AD%94%E3%81%88%E3%81%AF)(パラメータ 3 億,レイヤー 24 層) |
| 訓練のバッチサイズ(per_device_train_batch_size ) | 1                                                                                                                                                                                                                                                                                                                                                      |
| 評価のバッチサイズ(per_device_eval_batch_size)   |                                                                                                                                                                                                                                                                                                                                                        |
| 訓練のエポック数(num_train_epochs)               | 10                                                                                                                                                                                                                                                                                                                                                     |
| 何ステップ毎に保存するか(save_steps)             | 10000                                                                                                                                                                                                                                                                                                                                                  |
| save_total_limit                                 |                                                                                                                                                                                                                                                                                                                                                        |

実行は環境構築の都合上 Google Colab を用いて行っており、その都合で軽量の rinna/japanese-gpt2-medium を使っている。

### ログ

![ログ](https://user-images.githubusercontent.com/63891531/197667477-8ef24c54-5c8d-4d90-b7da-7e154f9aa420.jpg)

### 結果

生成結果の一例(2022 年 10 月 25 日の日付と日記本文を与えた)  
![結果](https://user-images.githubusercontent.com/63891531/197664065-25271426-e2a0-48dd-9af4-f5c3a59f9c9f.png)

- 日本語として自然な文章
- 文中で登場した人物がタイトルに含まれている
- 日記の本文に登場していない usuyuki の日記に入っていた人物や固有名詞が登場する
- 日記のタイトル生成時に、本文には登場していない usuyuki の個人情報が大量に登場する

以上の観点から、現状使える学習データでは個人情報が強く出てしまい、かどで日記の機能として実装することはできない。  
データ中の個人情報を強く反映するため、個々人の日記でモデルを作成するか、個人情報と判別できないレベルの大量の日記データでのモデル作成が必要だと思われる。

## 2.rinna/japanese-gpt2-medium を元に usuyuki 日記 2MB(本文＋タイトル)を学習させた場合

### 概要

1 では日付も入れていたが、日時による影響を受けすぎないように日時を入れないで学習させた。  
タイトルの長さは短くなったが、usuyuki の日記タイトルの傾向を引き継ぎつつも学習本データの特定の名詞などを出さず、与えた文章を反映した結果となった。  
一方で空文字列となることも増えた(これも usuyuki の日記にタイトルがない日記もあることを反映している可能性が高い)

```
igaito ikeru kamo
```

### 学習

タスク：

| タスク                                           | 日記の日付と本文からタイトルを生成する                                                                                                                                                                                                                                                                                                                 |
| :----------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| fine tuning に用いたデータ                       | usuyuki の 2018 年 1 月 10 日から 2022 年 10 月 24 日までの 1275 日記(日付、タイトル、本文)、UTF8 で 2MB 程度                                                                                                                                                                                                                                          |
| 根幹となるモデル                                 | GPT-2                                                                                                                                                                                                                                                                                                                                                  |
| pre training とした事前学習済モデル              | [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium?text=%E7%94%9F%E5%91%BD%E3%80%81%E5%AE%87%E5%AE%99%E3%80%81%E3%81%9D%E3%81%97%E3%81%A6%E4%B8%87%E7%89%A9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E7%A9%B6%E6%A5%B5%E3%81%AE%E7%96%91%E5%95%8F%E3%81%AE%E7%AD%94%E3%81%88%E3%81%AF)(パラメータ 3 億,レイヤー 24 層) |
| 訓練のバッチサイズ(per_device_train_batch_size ) | 1                                                                                                                                                                                                                                                                                                                                                      |
| 評価のバッチサイズ(per_device_eval_batch_size)   |                                                                                                                                                                                                                                                                                                                                                        |
| 訓練のエポック数(num_train_epochs)               | 10                                                                                                                                                                                                                                                                                                                                                     |
| 何ステップ毎に保存するか(save_steps)             | 10000                                                                                                                                                                                                                                                                                                                                                  |
| save_total_limit                                 |                                                                                                                                                                                                                                                                                                                                                        |

実行は環境構築の都合上 Google Colab を用いて行っており、その都合で軽量の rinna/japanese-gpt2-medium を使っている。

### ログ

![ログ](https://user-images.githubusercontent.com/63891531/197667972-2ab865be-19b2-482f-9067-a1bc1ac48fd9.png)

### 結果

生成結果の一例(2022 年 10 月 25 日の日付と日記本文を与えた)  
![結果](https://user-images.githubusercontent.com/63891531/197668625-f9f411e1-c8ca-4578-8043-e8ab25fe03c0.png)

- 空が多い(学習元データもタイトル空にしている日記が多数あり、学習を反映していると言える)
- 日記のタイトルとしては短いが、usuyuki の日記ではありうるパターン
- 特に日記で主軸となった人物名をタイトルにすることとかあり、自然

GPT というか、Transformer が入力 2 つ取るので、やはり本文＋何か入れる必要があるのかもしれない(これは Transformer に対しての認識違いの可能性があるため曖昧)。
日付を入れないほうが汎用的な結果となり、こちらであれば実用的なのかもしれない。

# 他

- 13 億パラメータが自慢の rinna/japanese-gpt-1b は Google Colab の VRAM 不足で実行不能。
- rinna/japanese-gpt2-small であればローカルの VRAM 容量でも学習できるが、あえてこれを利用する必要性はないため、モデルを作ったあと検証はしていないため掲載していない
